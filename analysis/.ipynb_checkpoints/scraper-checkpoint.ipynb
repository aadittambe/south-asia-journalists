{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "integrated-price",
   "metadata": {},
   "source": [
    "# journalists-scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-subcommittee",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from datetime import date\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import fnmatch\n",
    "import os\n",
    "import tabula\n",
    "from tabula.io import read_pdf\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from geopy.geocoders import Nominatim\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-merchant",
   "metadata": {},
   "source": [
    "## Download csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-diagram",
   "metadata": {},
   "source": [
    "#### Download the [csv](https://cpj.org/data/killed/?status=Killed&motiveConfirmed%5B%5D=Confirmed&type%5B%5D=Journalist&start_year=1992&end_year=2021&group_by=year) from the CPJ website about journalists who were killed and load it in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-former",
   "metadata": {},
   "outputs": [],
   "source": [
    "journalists_raw = pd.read_csv('journalists_killed.csv')\n",
    "journalists_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-wrestling",
   "metadata": {},
   "source": [
    "## Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-prospect",
   "metadata": {},
   "source": [
    "#### More information about each journalist can be retrieved from their bio page, which is in format 'https://cpj.org/data/people/< firstname-lastname >'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-wrong",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a list to store journalists' names\n",
    "names_list = []\n",
    "# Create a list of URLs that we want to scrape\n",
    "url_list = []\n",
    "# create a list of descriptions. We will use this to store the bios we retrieve.\n",
    "desc_list = []\n",
    "# Define base URL - the first part of the URL that's constant for every journalist's bio\n",
    "base_url = \"https://cpj.org/data/people/\"\n",
    "# Iterate through the dataframe to get each name\n",
    "for index, row in journalists_raw.iterrows():\n",
    "    names_list.append(row[\"fullName\"])\n",
    "# In order to get the desired URL, we have to add a dash (-) between first and last names\n",
    "for name in names_list:\n",
    "    name = name.replace(' ', '-')\n",
    "    # Add this to our base URL defined above\n",
    "    name_w_url = base_url + name\n",
    "    # Append url_list with this newly constructed URL\n",
    "    url_list.append(name_w_url)\n",
    "# Iterate through each URL in url_list\n",
    "for link in url_list:\n",
    "    url = link\n",
    "    # Open the URL\n",
    "    html = urllib.request.urlopen(url)\n",
    "    # Parse the html file\n",
    "    htmlParse = BeautifulSoup(html, 'html.parser')\n",
    "    # Iterate through all 'p' tags, clean them\n",
    "    for para in htmlParse.find_all('article', {\"class\":\"entry-content\"}):\n",
    "        para = para.text\n",
    "        para = para.replace('Share this:TwitterFacebookWhatsAppLinkedInEmailTelegram ', '')\n",
    "        para = para.strip()\n",
    "        para = para.replace('.\\n', '.<br>')\n",
    "        para = para.split('<br>')\n",
    "        # We want the first paragraph only, which tells us about the journalist; more cleaning\n",
    "        para = para[0]\n",
    "        para = para.replace('\\n','')\n",
    "        # Print for testing\n",
    "        print((para))\n",
    "        print('--')\n",
    "    desc_text = para\n",
    "    # Append these descriptions to the desc_list created above\n",
    "    desc_list.append(desc_text)\n",
    "journalists_with_desc = journalists_raw\n",
    "# Create a column in our original df, and store these descriptions for each journalist\n",
    "journalists_with_desc['desc'] = desc_list\n",
    "# View the df\n",
    "journalists_with_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-owner",
   "metadata": {},
   "source": [
    "#### Now, we have to add a new column to our df, which will have both city and country. This will be used to obtain coordinates for that location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-bibliography",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Journalists with city_country column\n",
    "\n",
    "journalists_city_country = journalists_with_desc\n",
    "journalists_city_country[\"city_country\"] = journalists_city_country[\"location\"] + \", \"+ journalists_city_country[\"country\"]\n",
    "\n",
    "journalists_city_country"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-agent",
   "metadata": {},
   "source": [
    "#### Some areas were listed as \"an area outside...\" or \"an area near...\" Take them out for consistency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naked-cancer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean that column using string detect\n",
    "journalists_city_country_clean = journalists_city_country\n",
    "journalists_city_country_clean['city_country'] = journalists_city_country_clean['city_country'].str.replace('an area outside ','')\n",
    "journalists_city_country_clean['city_country'] = journalists_city_country_clean['city_country'].str.replace('an area near ','')\n",
    "journalists_city_country_clean['city_country'] = journalists_city_country_clean['city_country'].str.replace('Mal√©','Male')\n",
    "\n",
    "\n",
    "# journalists_city_country_clean.sort_values(by = [\"city_country\"], ascending=True)\n",
    "journalists_city_country_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-sheep",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store this in a dataframe called jourcoords\n",
    "jourcoords = journalists_city_country_clean\n",
    "jourcoords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-fifth",
   "metadata": {},
   "source": [
    "#### Now, we use the geopy package to get coordinates. Some locations return errors, so we'll have to add coordinates manually. Add 'pass' if the function returns an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-projection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty list list called place_list\n",
    "place_list = []\n",
    "for place in jourcoords[\"city_country\"]:\n",
    "    place_list.append(place)\n",
    "\n",
    "# Create empty lists for latitude and longitude\n",
    "lat_list = []\n",
    "long_list = []\n",
    "for item in place_list:\n",
    "    # Try to get coordinates for locations\n",
    "    try: \n",
    "        geolocator = Nominatim(user_agent=\"Aadit\")\n",
    "        location = geolocator.geocode(item)\n",
    "        lat_list.append(location.latitude)\n",
    "        long_list.append(location.longitude)\n",
    "    # If the function errors out, return 'pass'\n",
    "    except (RuntimeError, TypeError, NameError, AttributeError):\n",
    "        lat_list.append('pass')\n",
    "        long_list.append('pass')\n",
    "# Print for testing\n",
    "print(len(long_list))\n",
    "print(len(lat_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-myrtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append lat and lon to jourcoords df\n",
    "jourcoords[\"lat\"] = lat_list\n",
    "jourcoords['lon'] = long_list\n",
    "jourcoords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-department",
   "metadata": {},
   "source": [
    "## Export csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-tsunami",
   "metadata": {},
   "source": [
    "#### Add coordinates for the four locations that returned 'pass'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-chrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "jourcoords.to_csv('final_file_clean.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
